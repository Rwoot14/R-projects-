---
title: "Assignment 1"
author: "Reece Wooten"
date: "11/4/2017"
output:
  html_document:
    theme: cerulean
    toc: yes
    toc_float: yes
  pdf_document:
    toc: yes
---


```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE)
library(fpp)
library(dplyr)
```

```{r echo=TRUE}

# read csv file and make it a time series object
RS<-read.csv(file="RSGCSN.csv") %>%
  select(-DATE) %>%
  ts(start=c(1992,1),frequency=12)

# training and test set split
tr<-window(RS,end=c(2011,12))
te<-window(RS,start=c(2012,1))
```

## Question 1
### Figure 1.0
```{r echo=TRUE}
# plotting full data
plot(RS)
abline(v=c(2011,12),col='grey')
```

```{r echo=TRUE}
# model fit and summary
f.HW<-ets(tr,model = 'AAM',restrict=FALSE,damped = NULL)
summary(f.HW)
```

```{r echo=TRUE}
# forecast
fc.HW<-forecast(f.HW,h = 68)
```

### Figure 1.1

```{r echo=TRUE}
# full forecast plot
plot(fc.HW)
lines(te,col='red',pch=19)
```

### Figure 1.2

```{r echo=TRUE}
# zoomed in forcast plot
plot(fc.HW,xlim=c(2009,2018),ylim = c(40000,60000))
lines(te,col='red',pch=19)
```

```{r echo=TRUE}
# test,training accuracy
accuracy(fc.HW,te)
```

### Analysis

* Looking at the output above, there is a training MAE of 412 and test MAE of 1340. When compared to the test MASE which is a ratio of the models MAE to the naive MAE, we get a value of 1.31 which indicates that the naive forecast actually does better than the models forecast. So we know that the naive MAE is less than 1340.

* The poor model performance is likely due to the restrictive nature of the models parameters. A more flexible model which is allowed to optimize its parameters will likely do better.

* I would likely choose a model which allowed the parameters to be optimized given the data, and any transformation of the data that would be needed.

## Question 2
```{r echo=TRUE}
# Model fit with damped=false
f.HW2<-ets(y = tr,model = 'AAM',restrict=FALSE,damped = FALSE)
summary(f.HW2)
```

### Figure 2.0

```{r echo=TRUE}
# Plot forecasts along with actuals
fc.HW2<-forecast(f.HW2,h = 68)
plot(fc.HW2,xlim=c(2009,2018),ylim = c(40000,60000))
lines(te,col='red',pch=19)
```

```{r echo=TRUE}
# test and training set accuracy
accuracy(fc.HW2,te)
```

### Analysis

* The out of sample MAE is 948, compared to the previous question with a MAE of 1340. This indicates that the second model has less bias than the first one, the MASE is also better than the first model with a value of .93. This indicates that the model does a slightly better job at forecasting than the naive method. 

* I suspect the reason why the confidence interval is much larger in the second problem is because we forced the damped term to be false. While it improved our point estimates, it lowered the confidence on the models fit.

* I would use model two's estimates because it has better measures of fit, including the RMSE, and MASE.

## Question 3
```{r echo=TRUE}
f.O<-ets(y = tr,model = 'ZZZ',restrict = FALSE)
summary(f.O)
```

### Figure 3.0

```{r echo=TRUE}
fc.O<-forecast(f.O,h=68)
plot(fc.O,xlim=c(2009,2018),ylim=c(40000,60000))
lines(te,col='red',pch=19)
```

```{r echo=TRUE}
accuracy(fc.O,te)
```

### Analysis

* The MAE for the test set is 824 which is better than both the previous models. Also the MASE is much better with a value of .66, which is significantly better than the naive forecast relative to the previous models.

* Model 3 has the best AICc and BIC of the three models, which indicates that it has the best fit to the data.
  + Model 3's AICc and BIC are respectively: 4354, 4411
  + Model 2's AICc and BIC are respectively: 4372, 4427
  + Model 1's AICc and BIC are respectively: 4369, 4429

* I would choose the current model to forecast due to the tighter confidence intervals, and the more accurate point forecasts, which is shown through the improved RMSE, and MASE.

## Question 4
```{r echo=TRUE}
L<-BoxCox.lambda(tr)
z<-BoxCox(tr,L)
par(mfrow=c(2,1))
plot(tr)
plot(z)
par(mfrow=c(1,1))
```

```{r echo=TRUE}
fB.O<-ets(y = tr,model = 'ZZZ',restrict = FALSE,lambda = L)
summary(fB.O)
```

### Figure 4.0

```{r echo=TRUE}
fBc.O<-forecast(fB.O,h = 68)
plot(fBc.O,xlim = c(2009,2018),ylim = c(40000,60000))
lines(te,col='red',pch=19)
```

```{r echo=TRUE}
accuracy(fBc.O,te)
```

### Analysis

* The out of sample MAE is 1086 and the MASE is 1.07 which indicates a reduction in fit compared to the previous model. The naive estimate does a better job at forecasting than the model does. This is likely because a box cox was fit to the entire data set when not all periods were needed.

* I would choose model fc.O because it has better MASE and RMSE out of sample scores.

## Question 5
```{r echo=TRUE}
fB.OD<-ets(y = tr,model = 'ZZZ',damped = TRUE,lambda = L)
summary(fB.OD)
```

### Figure 5.0

```{r echo=TRUE}
fBc.OD<-forecast(fB.OD,h=68)
plot(fBc.OD,xlim = c(2009,2018),ylim = c(40000,60000))
lines(te,col='red',pch=19)
```

```{r echo=TRUE}
accuracy(fBc.OD,te)

```

### Analysis

* The out of sample MAE and MASE were 2625 and 2.68 respectively. This is a significant loss in forecasting accuracy compared to the previous two models. The naive forecast does more than twice as good as the models forecast.

* The damping is likely not helping because the box cox transform is already correcting for the smoothing of the data set, add a damping factor to the trend only worsens the estimate.

* I would still choose model fc.O, it has outperformed each of these models in forecasting power.

## Question 6
```{r echo=TRUE}
lis<-list()
for(year in seq(1992,2006)){
  RS<-read.csv(file='RSGCSN.csv') %>%
  select(-DATE) %>%
  ts(start=c(1992,1),frequency=12)
  
  tr<-window(RS,start = c(year,1), end = c(2011,12))
  te<-window(RS,start=c(2012,1))
  
  L<-BoxCox.lambda(tr)
  fB.O<-ets(y = tr,model = 'ZZZ',restrict = FALSE,lambda = L)
  
  cat('\n-------------------------------------')
  
  cat('Starting Year:',year)
  
  cat('--------------------------------\n')
  
  a=accuracy(fB.O)['Training set','RMSE']
  cat('RMSE:',accuracy(fB.O)['Training set','RMSE'])
  lis[[year]]<-a
  
}


```

### Analysis

* The AICc and BIC are not comparable between models because they use different sets of data to fit the models. Only models which use the same data sets can be compared via AICc and BIC.

## Question 7
```{r echo=TRUE}
RS<-read.csv(file='RSGCSN.csv') %>%
  select(-DATE) %>%
  ts(start=c(1992,1),frequency=12)


trr<-window(RS,start=c(2003,1), end=c(2011,12))
te<-window(RS,start=c(2012,1))
```

```{r echo=TRUE}
L<-BoxCox.lambda(trr)
f<-ets(y = trr,model = 'ZZZ',restrict = FALSE,lambda = L)
summary(f)
```

### Figure 7.0

```{r echo=TRUE}
fc<-forecast(f,h = 68)
plot(fc,xlim = c(2009,2018))
lines(te,col='red',pch=19)
```

```{r echo=TRUE}
accuracy(fc,te)
```

### Analysis

* No, the AICc for model f.O and f can not compared. This is because the models were estimated by different data sets, and so there measures of fit are different.

* The MASE can be compared between f.O and f because it is a scale free measure of accuracy against the naive forecast.

* Because the data is the same between the two models forecasts and r transforms them back into the original units you can compare the two models RMSE.

* Because the model is only trained on the data before the forecast, the forecast is truly an out of sample forecast. 

## Question 8
```{r echo=TRUE}
RS<-read.csv(file='RSGCSN.csv') %>%
  select(-DATE) %>%
  ts(start=c(1992,1),frequency=12)


tr<-window(RS,start=c(2003,1),end=c(2017,8))
```

```{r echo=TRUE}
L<-BoxCox.lambda(tr)
ff<-ets(y = tr,model = 'ZZZ',restrict = FALSE,lambda = L)
summary(ff)
```

### Figure 8.0

```{r echo=TRUE}
fc<-forecast(ff,h = 64)
plot(fc)
```

### Analysis

* Since we cant compare the AICc and BIC, we will compare the RMSE and the MASE of the two models. The RMSE for model f is 494 and the MASE is .27 while the RMSE for ff is 532, and the MASE is .31. The model ff does slightly worse than the model f when compared against the measure of fit statistics. 

* I expect the out of sample RMSE to be similar to the in sample RMSE but slightly greater because the training data will be fit more accurately than the test data. Similarly, the MAPE should be similar to the in sample MAPE but slightly greater. These results can be shown in number 7 where we have a very similar model but with a test set. The RMSE and MAPE out of sample and in sample are very close to each other. 



